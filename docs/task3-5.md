# Task3
> From 无名C
## 心得&笔记&疑问&解答：

- [知识点] 从计算机视觉为起源的神经网络，核心架构三种：
    - FNN全连接：层内节点全连接
    - CNN卷积：中间层节点数比较少→抽象出特征？
    - RNN循环：输出作为输入

- [QA] Q: **三者的区别和架构区别的影响在哪？**
    > A: 
        - **FNN**：层内节点全连接，导致无法区分输入数据的空间/时序关系（如文本中词的顺序、图像中像素的位置），仅能学习全局特征映射，因此仅适用于简单分类（如MNIST手写数字），不适用于结构化数据（文本、图像）。
        - **CNN**：通过**卷积核**捕捉局部模式（如图像的边缘、文本的n-gram短语），再通过**池化**聚合全局信息，核心优势是“空间/局部结构建模”，但**无法处理时序依赖**（如文本中“他”指代前文的“小明”）。
        - **RNN**：循环结构→“时序依赖建模”——通过隐藏状态保存历史信息（如前一个词的语义），核心优势是“序列顺序感知”，但**并行性差、长距离依赖弱**。

- [QA] Q: 这句话怎么理解：“由于NLP任务所需要处理的文本往往是序列，因此专用于处理序列，时序数据的RNN往往能够在NLP任务上取得最优的效果”
    > A: →**顺序matters（“顺序决定语义”）所以是序列，但是RNN处理序列的优势怎么理解和推出来的？**
        >> A: RNN通过循环结构将历史信息编码进隐藏状态；每个时刻的输出依赖于“当前输入+所有历史输入的累积”,相当于浓缩的上下文。

- [知识点] RNN缺点：
    - RNN计算成本高
    - 序列相关的关系只能通过参数来调整，远距离关系易被忽视。

- [QA] Q: **门机制可以做哪些优化？**
    > A: 
    1. **遗忘门（Forget Gate）**：
    - 功能：决定上一时刻隐藏状态 \( h_{t-1} \) 中哪些信息被保留（如文本中“小明”的指代信息）、哪些被遗忘（无关冗余词）。
    - 优化点：避免远距离信息被“梯度消失”冲刷，强化长距离依赖。
    2. **输入门（Input Gate）**：
    - 功能：决定当前时刻输入 \( x_t \) 中哪些信息被纳入隐藏状态（如关键实体、逻辑词）。
    - 优化点：过滤噪声，聚焦核心信息，减少无效参数调整。
    3. **输出门（Output Gate）**：
    - 功能：决定隐藏状态中哪些信息输出到下一时刻（如当前词的语义是否需要传递给下一个词）。
    - 优化点：动态控制信息传递，避免无关信息干扰后续计算。
    4. **GRU的简化优化**：将遗忘门和输入门合并为“更新门”，保留“重置门”，在减少参数的同时保持长依赖建模能力，降低计算成本。

- [QA] Q: **注意力机制最初在cv领域是怎么被提出来的？又是怎么被迁移到NLP领域的？**
    > A: **模拟人类视觉的“选择性关注”**——人类观察图像时会聚焦关键区域（如人脸的眼睛），而非平等处理所有像素。（2014年《Recurrent Models of Visual Attention》（Mnih et al.），提出“硬注意力”（Hard Attention，直接选择部分区域）和“软注意力”（Soft Attention，对所有区域加权），用于解决图像分类、目标检测中的“信息过载”问题（如高分辨率图像的像素冗余）。）
    迁移的转折点：2017年Transformer论文《Attention Is All You Need》，首次将“缩放点积注意力”引入NLP，彻底替代RNN作为核心架构。
    CV中是“图像区域的关联”，NLP中是“token的关联”，本质都是“对结构化数据的选择性信息聚合”。

- [QA] Q：**注意力机制在CV领域是不是可以抽象理解**为画面中一个框？如果是，这个框和注意力机制的三个核心变量（Query、Key、Value）之间的关系是？
    > A: 可以这么理解，但不是一个严格的框。更准确的表述是热点图，**对图像中每个位置的“关注权重分布”**（软注意力）——权重高的区域相当于“框选的核心区域”，权重低的区域相当于“框外的次要区域”。
    与Q/K/V的对应关系
        - **Query（查询）**：当前任务目标（如“检测图像中的猫”“提取图像的边缘特征”），对应“关注框的选择标准”（比如“寻找猫的轮廓特征”）。
        - **Key（键）**：图像中每个区域的特征表示（如像素的纹理、局部卷积特征），对应“关注框的候选区域属性”（比如“某个区域是否有猫的耳朵特征”）。
        - **Value（值）**：图像中每个区域的原始/加工后特征（如像素值、高维卷积特征），对应“关注框内的实际信息内容”（比如“猫耳朵区域的具体像素数据”）。

- [QA] Q：类比NLP中呢？
    > A: 从某个词（query）出发对文本（Key/Value）的每一个token的相对注意分布；Query和Key的相关性

- [QA] 什么叫：通过计算Query与Key的相关性“为真值加权求和”
    > A: “真值”指Value（需要被聚合的原始信息，如NLP中的token向量、CV中的区域特征）；“加权求和”是根据Query与Key的相关性，给每个Value分配不同权重，再累加得到最终的注意力输出。
    #### 示例（NLP场景）
    假设文本是“小明喜欢吃苹果，他每天都买”，当前Query是“他”（想知道“他”指代谁）：
    1. Key是所有token的特征（“小明”“喜欢”“吃”“苹果”“他”“每天”“买”）；
    2. 计算Query（“他”）与每个Key的相关性：“小明”的相关性最高（权重0.8），其他token权重极低（如“喜欢”0.05、“苹果”0.03）；
    3. **Value是所有token的原始词向量；**
    4. 加权求和：0.8×“小明”的向量 + 0.05×“喜欢”的向量 + ... + 0.03×“苹果”的向量 → 最终输出聚焦于“小明”的特征向量，即“他”的注意力结果指向“小明”。

- [QA] 如何针对query，赋予每一个key权重？（对于每个query，每个key的权重都不同吗？）
    > A: 都不同。

- [知识点] 词向量：词义相近的词在向量空间中距离更近；距离：欧氏距离or**向量点积**

- [QA] Q: 点积的几何意义？相似大于0，不相似小于0？）
    > A: 
    点积的公式为：
    $$a \cdot b = |a||b|\cos\theta$$
    其中 \( \theta \) 是两向量的夹角，其本质是：
    > “一个向量在另一个向量方向上的投影长度，乘以另一个向量的模长”
    ### 点积与向量方向的关系
    - 当两向量方向一致，点积达到最大值；
    - 当两向量垂直，点积为0；
    - 当两向量方向相反，点积达到最小值。
    ### 相似性判断
    点积的正负**不直接等同于“相似/不相似”**，而是反映“向量方向是否一致”：
    - 点积>0：两向量夹角<90°，方向大致相同（语义倾向一致，如“猫”和“狗”都是动物）；
    - 点积=0：两向量垂直，无直接关联（如“猫”和“桌子”）；
    - 点积<0：两向量夹角>90°，方向相反（语义倾向相反，如“喜欢”和“讨厌”）。
    ⚠️ 注意：
    相似性需结合向量模长判断——**将向量归一化后（\( |a|=|b|=1 \)）**，点积直接等于 \( \cos\theta \)（取值范围[-1,1]），此时“点积越大，向量的相似性越高”。

- [QA] Q：为什么最终的注意力计算要乘以V？
    > A: 
    1. Query和Key的作用：仅用于计算“哪些信息重要”（权重），但它们本身不包含“需要输出的具体内容”；
    2. Value的作用：存储每个位置的原始/加工后特征（如NLP中的词语义、CV中的图像区域细节），是“权重作用的对象”；
    3. 乘法的意义：将“重要性权重”与“实际信息”绑定——高权重的Value（重要信息）在输出中占比更高，低权重的Value（次要信息）占比更低，最终输出是“筛选后的有效信息聚合”;
    4. 反例（如果不乘V）
    > 若仅用Query与Key的权重输出，相当于“只知道哪些信息重要，但没提取这些信息的内容”，无法形成有意义的特征表示（比如只知道“小明”与“他”相关，但没得到“小明”的语义向量，就无法完成指代消解任务）。
- [QA] Q：怎么理解“如果Q和K对应的维度dk比较大，softmax放缩时非常容易受影响”
    > A: ![豆包的解释](./pics/cap01.png)
   
- [知识点] 例如在 Transformer 的 Decoder 结构中，Q 来自于 Decoder 的输入，K 与 V 来自于 Encoder 的输出，从而拟合了编码信息与历史信息之间的关系，便于综合这两种信息实现未来的预测。

- [QA] Q：Decoder/Encoder是怎么提出来的？提出的初衷是什么？
    > A: 
    - 2014 年《Sequence to Sequence Learning with Neural Networks》（Sutskever et al.）首次提出 “Encoder-Decoder 架构”，将任务拆分为 “理解输入” 和 “生成输出” 两个独立但联动的阶段；
    - 提出的核心初衷，是为了解决 **“序列到序列（Seq2Seq）” 任务 ** 的核心矛盾 ——“输入序列的理解” 与 “输出序列的生成” 需要不同的建模逻辑：
        - Encoder 的初衷：专注于 “理解输入序列的全局语义”（如机器翻译中 “源语言句子的含义”），无需考虑输出端的生成顺序。因此 Encoder 是 “双向建模”（可看到所有输入 token），核心目标是将输入序列编码为一个结构化的 “语义向量”（或特征矩阵）；
        - Decoder 的初衷：专注于 “基于输入语义生成符合规则的输出序列”（如机器翻译中 “目标语言句子”），需要考虑输出的时序依赖（如生成第 t 个词时，只能依赖前 t-1 个词）。因此 Decoder 是 “单向建模”（通过掩码遮蔽未来 token），核心目标是从 Encoder 的语义输出中逐步生成目标序列。
    - 典型场景举例
        - 机器翻译：Encoder 处理 “中文输入”（如 “我喜欢学习”），Decoder 生成 “英文输出”（如 “I like studying”）；
        - 文本摘要：Encoder 理解 “长文本原文”，Decoder 生成 “短文本摘要”；
        - 对话生成：Encoder 理解 “用户问句”，Decoder 生成 “机器人回复”。

- [知识点] 自注意力（self-attention，自注意力）机制。所谓自注意力，即是计算本身序列中每个元素对其他元素的注意力分布，即在计算过程中，Q、K、V 都由同一个输入通过不同的参数矩阵计算得到。在 Encoder 中，Q、K、V 分别是输入对参数矩阵 $W_q、W_k、W_v$ 做积得到，从而拟合输入语句中每一个 token 对其他所有 token 的关系。

- [知识点] 掩码自注意力，即 Mask Self-Attention，是指使用注意力掩码的自注意力机制。掩码的作用是遮蔽一些特定位置的 token，模型在学习的过程中，会忽略掉被遮蔽的 token。

- [知识点] Transformer的机制是即对一个文本序列，不断根据之前的 token 来预测下一个 token，直到将整个文本序列补全。
​
- [QA] Q：掩码自注意力是学习过程中才用到的吗？
    > A: 不是，训练和推理阶段都需要用到，但作用场景和掩码逻辑有差异。训练时强调单向，推理时基于已生成词预测下一个词。

- [QA] Q：RNN为什么可以并行计算？
    > A: 标准RNN因为时序依赖是无法并行计算的。是Transformer的掩码自注意力实现序列计算的并行化。


- [QA] Q：意思是不是一句话拆成5句去输入？
    > A: 是的，多个独立的序列（如批次中的不同句子）可以同时处理，但单个序列内部的时刻计算仍需串行。目的是提升训练速度。
        > 针对这个问题，Transformer 就提出了掩码自注意力的方法。掩码自注意力会生成一串掩码，来遮蔽未来信息。例如，我们待学习的文本序列仍然是 【BOS】I like you【EOS】，我们使用的注意力掩码是【MASK】，那么模型的输入为：

            <BOS> 【MASK】【MASK】【MASK】【MASK】
            <BOS>    I   【MASK】 【MASK】【MASK】
            <BOS>    I     like  【MASK】【MASK】
            <BOS>    I     like    you  【MASK】
            <BOS>    I     like    you   </EOS>

        > 在每一行输入中，模型仍然是只看到前面的 token，预测下一个 token。但是注意，上述输入不再是串行的过程，而可以一起并行地输入到模型中，模型只需要每一个样本根据未被遮蔽的 token 来预测下一个 token 即可，从而实现了并行的语言模型。

- [知识点] 在注意力计算时，我们会将计算得到的注意力分数与这个掩码做和，再进行 Softmax 操作。
- [QA] Q：这个操作的实际意义/影响是什么？
    > A: 通过做求和，上三角区域（也就是应该被遮蔽的 token 对应的位置）的注意力分数结果都变成了 -inf，而下三角区域的分数不变。再做 Softmax 操作，-inf 的值在经过 Softmax 之后会被置为 0，从而忽略了上三角区域计算的注意力分数，从而实现了注意力遮蔽。

- [知识点] 但一次注意力计算只能拟合一种相关关系，单一的注意力机制很难全面拟合语句序列里的相关关系。
- [QA] Q：是不是通过不同的初始化值去拟合不同的相关关系？那会不会出现多次计算最终拟合到了同一种关系？
    > A: 是。初始化时通常使用 “随机正交初始化”，确保不同头的初始投影矩阵差异较大；其他手段：如“参数冗余设置隐性惩罚”。

- [知识点] 注意力机制及多头注意力的代码实现：
    > [代码实现](./code/happy-llm.ipynb)



