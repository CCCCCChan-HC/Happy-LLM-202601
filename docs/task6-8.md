# Task6
### 心得&笔记&疑问&解答：
## Encoder-Only PLM
**[知识点]** 针对 Encoder、Decoder 的特点，**引入 ELMo 的预训练思路**，开始出现不同的、对 Transformer 进行优化的思路。
- [QA] Q：引入ELMo预训练思路 此处是什么意思？
    > A: ELMo 的诞生标志着预训练+微调范式的诞生。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，将 NLP 领域导向预训练+微调的研究思路。
### BERT
**[知识点]** BERT，全名为 Bidirectional Encoder Representations from Transformers，发布于论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》
**[知识点]** 出现了如 MacBERT、BART 等基于 BERT 进行优化提升的模型。


#### 沿承的核心思想包括：
- Transformer 架构。通过将 Encoder 结构进行堆叠，扩大模型参数
- 预训练+微调范式。掩码语言模型（Masked Language Model，MLM），能捕捉深层双向语义关系
#### 模型架构—Encoder Only
![BERT模型结构](./pics/cap02.png)
**[知识点]** 没有加入对特定任务的 Decoder。为适配各种 NLU 任务，在模型的最顶层加入了一个分类头 prediction_heads，用于将多维度的隐藏状态通过线性层转换到分类维度（例如，如果一共有两个类别，prediction_heads 输出的就是两维向量）

**[知识点]** BERT 有两种规模的模型，分别是 base 版本（12层 Encoder Layer，768 的隐藏层维度，总参数量 110M），large 版本（24层 Encoder Layer，1024 的隐藏层维度，总参数量 340M）。通过Encoder 编码之后的最顶层 hidden_states 最后经过 prediction_heads 就得到了最后的类别概率，经过 Softmax 计算就可以计算出模型预测的类别。

> BERT 采用 WordPiece 作为分词方法。WordPiece 是一种基于统计的子词切分算法，其核心在于将单词拆解为子词（例如，"playing" -> ["play", "##ing"]）。其合并操作的依据是最大化语言模型的似然度。对于中文等非空格分隔的语言，通常将单个汉字作为原子分词单位（token）处理。

**[知识点]** BERT 所使用的激活函数是 GELU 函数，全名为高斯误差线性单元激活函数.GELU 的计算方式为：

$$GELU(x) = 0.5x(1 + tanh(\sqrt{\frac{2}{\pi}})(x + 0.044715x^3))$$

**[知识点]** GELU 的核心思路为将随机正则的思想引入激活函数，通过输入自身的概率分布，来决定抛弃还是保留自身的神经元。
- [QA] Q：怎么理解GELU的核心思想？
    > A: 
    1. 传统激活函数（比如 ReLU）：是 “固定开关”—— 输入大于 0，员工就干活；输入小于 0，员工就摸鱼（不管其他情况）；
    2. GELU 激活函数：是 “随机开关”—— 员工是否干活，由 “输入的概率” 决定：
    比如输入是 “写报告”，GELU 会先算 “这个输入需要员工干活的概率”（比如概率 = 0.8）；
    然后扔一个 “概率骰子”：如果骰子结果≤0.8，员工就干活；如果 > 0.8，员工就摸鱼；
    而且这个 “概率” 是根据输入自身的大小动态变化的—— 输入越重要（数值越大），干活的概率越高；输入越不重要（数值越小），干活的概率越低。
    3. 核心思想：
    GELU 把 “随机正则（比如 Dropout）” 和 “激活函数” 合二为一 —— 既让模型 “选择性激活神经元”（像 ReLU 一样做特征筛选），又让模型 “随机抛弃部分神经元”（像 Dropout 一样防止过拟合），相当于 “让员工既按能力干活，又随机轮休”，既提高效率又避免疲劳（过拟合）。

**[知识点]** BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数
- [QA] Q：为什么这么做？
    > A: 通过可训练的参数来拟合相对位置，相对而言比 Transformer 使用的绝对位置编码 Sinusoidal 能够拟合更丰富的相对位置信息，但是，这样也增加了不少模型参数，同时完全无法处理超过模型训练长度的输入（例如，对 BERT 而言能处理的最大上下文长度是 512 个 token）。

#### 预训练任务-MLM+NSP
**[知识点]** NSP（Next Sentence Prediction，下一句预测）
**[知识点]** 通过将预训练和微调分离，完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上。
-> 因此，可以进一步扩大模型参数和预训练数据量。
-> 预训练数据的核心要求即是需要极大的数据规模（数亿 token）
-> 预训练数据一定是从无监督的语料中获取
-> 统的预训练任务都是 LM 的原因(LM 使用上文预测下文的方式可以直接应用到任何文本中)
-> LM 预训练任务的一大缺陷在于，其直接拟合从左到右的语义关系，但忽略了双向的语义关系
-> MLM诞生

**[知识点]** MLM 模拟的是“完形填空”
- 在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token
- MLM存在其固有缺陷。在下游任务微调和推理时，其实是不存在我们人工加入的\<MASK\>的。

**[知识点]** 对 MLM 的策略进行了改进：
- 随机选择训练语料中 15% 的 token 用于遮蔽
- 80% 的概率被遮蔽
- 10% 的概率被替换为任意一个 token（核心意义在于迫使模型保持对上下文信息的学习。因为如果全部遮蔽的话，模型仅需要处理被遮蔽的位置，从而仅学习要预测的 token 而丢失了对上下文的学习。通过引入部分随机 token，模型无法确定需要预测的 token，从而被迫保持每一个 token 的上下文表征分布，从而具备了对句子的特征表示能力。且由于随机 token 的概率很低，其并不会影响模型实质的语言理解能力。）
- 10% 的概率保持不变（为了消除预训练和微调的不一致）

**[知识点]** NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文
**[知识点]** NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行）

**[知识点]** 训练的超参数也是值得关注的，BERT 的训练语料共有 13GB 大小，其在 256 的 batch size 上训练了 1M 步（40 个 Epoch）。而相较而言，LLM 一般都只会训练一个 Epoch，且使用远大于 256 的 batch size。
- [QA] Q: 这句话怎么理解？
> A: 类比就是，BERT 预训练时，每次做 256 道题，改完错再写下一批，一共做40批；而LLM一般只做一批，但是题目量远大于BERT

#### 下游任务微调
**[知识点]** BERT 设计了更通用的输入和输出层来适配多任务下的迁移学习。对每一个输入的文本序列，BERT 会在其首部加入一个特殊 token \<CLS\>。在后续编码中，该 token 代表的即是整句的状态

**[知识点]** 所谓微调，其实和训练时更新模型参数的策略一致，只不过在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。

### RoBERTa 
由 Facebook 发布
#### 优化一：去掉 NSP 预训练任务
**[知识点]** 设置了四个实验组：

    1. 段落构建的 MLM + NSP：BERT 原始预训练任务，输入是一对片段，每个片段包括多个句子，来构造 NSP 任务；
    2. 文档对构建的 MLM + NSP：一个输入构建一对句子，通过增大 batch 来和原始输入达到 token 等同；
    3. 跨越文档的 MLM：去掉 NSP 任务，一个输入为从一个或多个文档中连续采样的完整句子，为使输入达到最大长度（512），可能一个输入会包括多个文档；
    4. 单文档的 MLM：去掉 NSP 任务，且限制一个输入只能从一个文档中采样，同样通过增大 batch 来和原始输入达到 token 等同

后两组显著优于前两组，RoBERTa 在预训练中去掉了 NSP

**[知识点]** 动态遮蔽策略，让每一个 Epoch 的训练数据 Mask 的位置都不一致
#### 优化二：更大规模的预训练数据和预训练步长
**[知识点]** RoBERTa 认为更大的 batch size 既可以提高优化速度，也可以提高任务结束性能。
- [QA] Q: 为什么这么做？更大的预训练数据、更大的预训练步长的重要意义？
> A: 
1. 更大 batch size 的好处（对应 “每次多做几道题”）：
提高优化速度：
普通训练（batch size=256）= 每次做 256 道题，改完错再写下一批；
大 batch（比如 batch size=8192）= 每次做 8192 道题，改一次错就能覆盖更多题型，模型 “学知识的效率变高”，不用重复做同类题。
提高最终性能：
大 batch 能一次性看到更多 “不同类型的题”（比如同时练语文、数学、英语），模型学到的 “知识更全面”，不会偏科 —— 相当于你刷题时一次做语数外各 10 道，比每次只做 10 道数学，知识覆盖更广。
2. 更大预训练数据的意义（对应 “用更多题本刷题”）：
普通 BERT 用 13GB 语料 = 只刷一本《高考必刷题》；
大模型用 TB 级语料 = 刷《高考必刷题 + 历年真题 + 模拟卷 + 课外拓展题》—— 题越多，模型见的 “语言现象”（比如不同句式、生僻词、逻辑关系）越全，学到的 “语言能力” 越强，做新题（下游任务）时更会举一反三。
3. 更长预训练步长的意义（对应 “刷更多轮题”）：
步长 =“刷一轮题” 的次数：BERT 刷 1M 步 = 把题本做了 40 遍；
更长步长 = 把题本刷更多遍 —— 模型能把 “知识从记不住→记住→熟练→融会贯通”，比如第一次刷题只懂 “主谓宾”，刷 10 遍后能懂 “倒装句、省略句”，刷 100 遍后能熟练用复杂句式。
#### 优化三：更大的 bpe 词表
**[知识点]** BPE，即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。以字为基本单位的中文，一般会按照字节编码进行切分。
**[知识点]** 一般来说，BPE 编码的词典越大，编码效果越好。
### ALBERT
### 优化一：将 Embedding 参数进行分解
**[知识点]** Embedding 层的参数矩阵维度为 $V*H$，此处的 V 为词表大小 30K，H 即为隐藏层大小 1024，也就是 Embedding 层参数达到了 30M。
**[知识点]** Embedding 层输出的向量是我们对文本 token 的稠密向量表示，从 Word2Vec来看，这种词向量并不需要很大的维度。
**[知识点]** 在 Embedding 层的后面加入一个线性矩阵进行维度变换。ALBERT 设置了 Embedding 层的输出为 128，因此在 Embedding 层后面加入了一个 $128 * 1024$ 的线性矩阵来将 Embedding 层的输出再升维到隐藏层大小。

### 优化二：跨层进行参数共享
**[知识点]** 让各个 Encoder 层共享模型参数，来减少模型的参数量。
**[知识点]** 仅初始化了一个Encoder层。在计算过程中，仍然会进行24次计算，但是每一次计算都是经过这一个Encoder层。因此，虽然是24个Encoder 计算的模型，但只有一层 Encoder参数，从而大大降低了模型参数量。在这样的情况下，就可以极大程度地扩大隐藏层维度，实现一个更宽但参数量更小的模型。

**[知识点]** 不足：训练和推理时的速度相较 BERT 还会更慢

#### （3）优化三：提出 SOP 预训练任务

**[知识点]** 改进 NSP，增加其难度，优化模型的预训练

**[知识点]** 改进是，正例同样由两个连续句子组成，但负例是将这两个的顺序反过来。模型不仅要拟合两个句子之间的关系，更要学习其顺序关系

**[知识点]** 还有许多从其他更高角度对 BERT 进行优化的后起之秀，包括进一步改进了预训练任务的 ERNIE、对 BERT 进行蒸馏的小模型 DistilBERT、主打多语言任务的 XLM 等，本文就不再一一赘述。

