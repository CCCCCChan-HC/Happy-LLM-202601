# Task6
### 心得&笔记&疑问&解答：
## Encoder-Only PLM
**[知识点]** 针对 Encoder、Decoder 的特点，**引入 ELMo 的预训练思路**，开始出现不同的、对 Transformer 进行优化的思路。
- [QA] Q：引入ELMo预训练思路 此处是什么意思？
    > A: ELMo 的诞生标志着预训练+微调范式的诞生。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，将 NLP 领域导向预训练+微调的研究思路。
### BERT
**[知识点]** BERT，全名为 Bidirectional Encoder Representations from Transformers，发布于论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》
**[知识点]** 出现了如 MacBERT、BART 等基于 BERT 进行优化提升的模型。


#### 沿承的核心思想包括：
- Transformer 架构。通过将 Encoder 结构进行堆叠，扩大模型参数
- 预训练+微调范式。掩码语言模型（Masked Language Model，MLM），能捕捉深层双向语义关系
#### 模型架构—Encoder Only
![BERT模型结构](./pics/cap02.png)
**[知识点]** 没有加入对特定任务的 Decoder。为适配各种 NLU 任务，在模型的最顶层加入了一个分类头 prediction_heads，用于将多维度的隐藏状态通过线性层转换到分类维度（例如，如果一共有两个类别，prediction_heads 输出的就是两维向量）

**[知识点]** BERT 有两种规模的模型，分别是 base 版本（12层 Encoder Layer，768 的隐藏层维度，总参数量 110M），large 版本（24层 Encoder Layer，1024 的隐藏层维度，总参数量 340M）。通过Encoder 编码之后的最顶层 hidden_states 最后经过 prediction_heads 就得到了最后的类别概率，经过 Softmax 计算就可以计算出模型预测的类别。

> BERT 采用 WordPiece 作为分词方法。WordPiece 是一种基于统计的子词切分算法，其核心在于将单词拆解为子词（例如，"playing" -> ["play", "##ing"]）。其合并操作的依据是最大化语言模型的似然度。对于中文等非空格分隔的语言，通常将单个汉字作为原子分词单位（token）处理。

**[知识点]** BERT 所使用的激活函数是 GELU 函数，全名为高斯误差线性单元激活函数.GELU 的计算方式为：

$$GELU(x) = 0.5x(1 + tanh(\sqrt{\frac{2}{\pi}})(x + 0.044715x^3))$$

**[知识点]** GELU 的核心思路为将随机正则的思想引入激活函数，通过输入自身的概率分布，来决定抛弃还是保留自身的神经元。
- [QA] Q：怎么理解GELU的核心思想？
    > A: 
    1. 传统激活函数（比如 ReLU）：是 “固定开关”—— 输入大于 0，员工就干活；输入小于 0，员工就摸鱼（不管其他情况）；
    2. GELU 激活函数：是 “随机开关”—— 员工是否干活，由 “输入的概率” 决定：
    比如输入是 “写报告”，GELU 会先算 “这个输入需要员工干活的概率”（比如概率 = 0.8）；
    然后扔一个 “概率骰子”：如果骰子结果≤0.8，员工就干活；如果 > 0.8，员工就摸鱼；
    而且这个 “概率” 是根据输入自身的大小动态变化的—— 输入越重要（数值越大），干活的概率越高；输入越不重要（数值越小），干活的概率越低。
    3. 核心思想：
    GELU 把 “随机正则（比如 Dropout）” 和 “激活函数” 合二为一 —— 既让模型 “选择性激活神经元”（像 ReLU 一样做特征筛选），又让模型 “随机抛弃部分神经元”（像 Dropout 一样防止过拟合），相当于 “让员工既按能力干活，又随机轮休”，既提高效率又避免疲劳（过拟合）。

**[知识点]** BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数
- [QA] Q：为什么这么做？
    > A: 通过可训练的参数来拟合相对位置，相对而言比 Transformer 使用的绝对位置编码 Sinusoidal 能够拟合更丰富的相对位置信息，但是，这样也增加了不少模型参数，同时完全无法处理超过模型训练长度的输入（例如，对 BERT 而言能处理的最大上下文长度是 512 个 token）。

#### 预训练任务-MLM+NSP
**[知识点]** NSP（Next Sentence Prediction，下一句预测）
**[知识点]** 通过将预训练和微调分离，完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上。
-> 因此，可以进一步扩大模型参数和预训练数据量。
-> 预训练数据的核心要求即是需要极大的数据规模（数亿 token）
-> 预训练数据一定是从无监督的语料中获取
-> 统的预训练任务都是 LM 的原因(LM 使用上文预测下文的方式可以直接应用到任何文本中)
-> LM 预训练任务的一大缺陷在于，其直接拟合从左到右的语义关系，但忽略了双向的语义关系
-> MLM诞生

**[知识点]** MLM 模拟的是“完形填空”
- 在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token
- MLM存在其固有缺陷。在下游任务微调和推理时，其实是不存在我们人工加入的\<MASK\>的。

**[知识点]** 对 MLM 的策略进行了改进：
- 随机选择训练语料中 15% 的 token 用于遮蔽
- 80% 的概率被遮蔽
- 10% 的概率被替换为任意一个 token（核心意义在于迫使模型保持对上下文信息的学习。因为如果全部遮蔽的话，模型仅需要处理被遮蔽的位置，从而仅学习要预测的 token 而丢失了对上下文的学习。通过引入部分随机 token，模型无法确定需要预测的 token，从而被迫保持每一个 token 的上下文表征分布，从而具备了对句子的特征表示能力。且由于随机 token 的概率很低，其并不会影响模型实质的语言理解能力。）
- 10% 的概率保持不变（为了消除预训练和微调的不一致）

**[知识点]** NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文
**[知识点]** NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行）

**[知识点]** 训练的超参数也是值得关注的，BERT 的训练语料共有 13GB 大小，其在 256 的 batch size 上训练了 1M 步（40 个 Epoch）。而相较而言，LLM 一般都只会训练一个 Epoch，且使用远大于 256 的 batch size。
- [QA] Q: 这句话怎么理解？
> A: 类比就是，BERT 预训练时，每次做 256 道题，改完错再写下一批，一共做40批；而LLM一般只做一批，但是题目量远大于BERT

#### 下游任务微调
**[知识点]** BERT 设计了更通用的输入和输出层来适配多任务下的迁移学习。对每一个输入的文本序列，BERT 会在其首部加入一个特殊 token \<CLS\>。在后续编码中，该 token 代表的即是整句的状态

**[知识点]** 所谓微调，其实和训练时更新模型参数的策略一致，只不过在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。

### RoBERTa 
由 Facebook 发布
#### 优化一：去掉 NSP 预训练任务
**[知识点]** 设置了四个实验组：

    1. 段落构建的 MLM + NSP：BERT 原始预训练任务，输入是一对片段，每个片段包括多个句子，来构造 NSP 任务；
    2. 文档对构建的 MLM + NSP：一个输入构建一对句子，通过增大 batch 来和原始输入达到 token 等同；
    3. 跨越文档的 MLM：去掉 NSP 任务，一个输入为从一个或多个文档中连续采样的完整句子，为使输入达到最大长度（512），可能一个输入会包括多个文档；
    4. 单文档的 MLM：去掉 NSP 任务，且限制一个输入只能从一个文档中采样，同样通过增大 batch 来和原始输入达到 token 等同

后两组显著优于前两组，RoBERTa 在预训练中去掉了 NSP

**[知识点]** 动态遮蔽策略，让每一个 Epoch 的训练数据 Mask 的位置都不一致
#### 优化二：更大规模的预训练数据和预训练步长
**[知识点]** RoBERTa 认为更大的 batch size 既可以提高优化速度，也可以提高任务结束性能。
- [QA] Q: 为什么这么做？更大的预训练数据、更大的预训练步长的重要意义？
> A: 
1. 更大 batch size 的好处（对应 “每次多做几道题”）：
提高优化速度：
普通训练（batch size=256）= 每次做 256 道题，改完错再写下一批；
大 batch（比如 batch size=8192）= 每次做 8192 道题，改一次错就能覆盖更多题型，模型 “学知识的效率变高”，不用重复做同类题。
提高最终性能：
大 batch 能一次性看到更多 “不同类型的题”（比如同时练语文、数学、英语），模型学到的 “知识更全面”，不会偏科 —— 相当于你刷题时一次做语数外各 10 道，比每次只做 10 道数学，知识覆盖更广。
2. 更大预训练数据的意义（对应 “用更多题本刷题”）：
普通 BERT 用 13GB 语料 = 只刷一本《高考必刷题》；
大模型用 TB 级语料 = 刷《高考必刷题 + 历年真题 + 模拟卷 + 课外拓展题》—— 题越多，模型见的 “语言现象”（比如不同句式、生僻词、逻辑关系）越全，学到的 “语言能力” 越强，做新题（下游任务）时更会举一反三。
3. 更长预训练步长的意义（对应 “刷更多轮题”）：
步长 =“刷一轮题” 的次数：BERT 刷 1M 步 = 把题本做了 40 遍；
更长步长 = 把题本刷更多遍 —— 模型能把 “知识从记不住→记住→熟练→融会贯通”，比如第一次刷题只懂 “主谓宾”，刷 10 遍后能懂 “倒装句、省略句”，刷 100 遍后能熟练用复杂句式。
#### 优化三：更大的 bpe 词表
**[知识点]** BPE，即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。以字为基本单位的中文，一般会按照字节编码进行切分。
**[知识点]** 一般来说，BPE 编码的词典越大，编码效果越好。
### ALBERT
### 优化一：将 Embedding 参数进行分解
**[知识点]** Embedding 层的参数矩阵维度为 $V*H$，此处的 V 为词表大小 30K，H 即为隐藏层大小 1024，也就是 Embedding 层参数达到了 30M。
**[知识点]** Embedding 层输出的向量是我们对文本 token 的稠密向量表示，从 Word2Vec来看，这种词向量并不需要很大的维度。
**[知识点]** 在 Embedding 层的后面加入一个线性矩阵进行维度变换。ALBERT 设置了 Embedding 层的输出为 128，因此在 Embedding 层后面加入了一个 $128 * 1024$ 的线性矩阵来将 Embedding 层的输出再升维到隐藏层大小。

### 优化二：跨层进行参数共享
**[知识点]** 让各个 Encoder 层共享模型参数，来减少模型的参数量。
**[知识点]** 仅初始化了一个Encoder层。在计算过程中，仍然会进行24次计算，但是每一次计算都是经过这一个Encoder层。因此，虽然是24个Encoder 计算的模型，但只有一层 Encoder参数，从而大大降低了模型参数量。在这样的情况下，就可以极大程度地扩大隐藏层维度，实现一个更宽但参数量更小的模型。

**[知识点]** 不足：训练和推理时的速度相较 BERT 还会更慢

#### （3）优化三：提出 SOP 预训练任务

**[知识点]** 改进 NSP，增加其难度，优化模型的预训练

**[知识点]** 改进是，正例同样由两个连续句子组成，但负例是将这两个的顺序反过来。模型不仅要拟合两个句子之间的关系，更要学习其顺序关系

**[知识点]** 还有许多从其他更高角度对 BERT 进行优化的后起之秀，包括进一步改进了预训练任务的 ERNIE、对 BERT 进行蒸馏的小模型 DistilBERT、主打多语言任务的 XLM 等，本文就不再一一赘述。

# Task 7
## Encoder-Decoder PLM
**[知识点]** BERT 也存在一些问题，例如 MLM 任务和下游任务微调的不一致性，以及无法处理超过模型训练长度的输入等问题。
- [QA] Q: 展开讲讲问题和原因
> A: 
1. 预训练和微调的 “输入规则” 不一样
2. MLM 任务只要求模型根据上下文预测单个token，关注的是局部上下文的语义关联；而下游任务（如文本摘要、问答）往往需要全局语义理解、长距离依赖建模，两者的学习目标存在偏差。
3. 静态掩码
4. 输入长度问题见task 6.

### T5
**[知识点]** T5（Text-To-Text Transfer Transformer）是由 Google 提出,将所有 NLP 任务统一表示为文本到文本的转换问题
#### 模型结构：Encoder-Decoder
**[知识点]** 编码器用于处理输入文本，解码器用于生成输出文本。编码器和解码器之间通过注意力机制进行信息交互.
![T5 模型详细结构](./pics/t5.png)
**[知识点]** Transformer 部分又分为 EncoderLayers 和 DecoderLayers 两部分，由小的Block组成，每个 Block 包含了多头注意力机制、前馈神经网络和 Norm 层。Block 的设计可以使模型更加灵活，根据任务的复杂程度和数据集的大小来调整 Block 的数量和层数。

**[知识点]** 前馈神经网络用于处理特征的非线性变换
- [QA] Q: 这句话怎么具体理解
> A: 在自注意力机制提取的 “注意力特征” 基础上，通过线性变换+非线性激活的组合，让模型突破线性表达的局限，学习到文本中更复杂的语义、语法和逻辑关系。

**[知识点]** 在 Decoder 中还包含了 Encoder-Decoder Attention 结构，在位置编码和 Mask 机制上有所不同。
![Encoder和Decoder](./pics/encoder-decoder.png)

**[知识点]** 与原始 Transformer 模型不同，T5 模型的LayerNorm 采用了 RMSNorm，通过计算每个神经元的均方根（Root Mean Square）来归一化每个隐藏层的激活值。RMSNorm 的参数设置与Layer Normalization 相比更简单，只有一个可学参数，可以更好地适应不同的任务和数据集。RMSNorm函数可以用以下数学公式表示：

$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2 + \epsilon}} \cdot \gamma
$$

其中：
- $x_i$ 是输入向量的第 $i$ 个元素
- $\gamma$ 是可学习的缩放参数
- $n$ 是输入向量的维度数量
- $\epsilon$ 是一个小常数，用于数值稳定性（以避免除以零的情况）

这种归一化有助于通过确保权重的规模不会变得过大或过小来稳定学习过程，这在具有许多层的深度学习模型中特别有用。

- [QA] Q: 对比一下LayerNorm
> A: **核心简化点**：
- 去掉了**减去均值**的步骤，只做“标准化”，不做“中心化”
- 只有 **1个可学习参数**：$\gamma$（缩放因子），没有平移参数 $\beta$
    #### LayerNorm 与 RMSNorm 的核心区别对比
    | 对比维度 | 原始Transformer的LayerNorm | T5的RMSNorm |
    | :--- | :--- | :--- |
    | **核心计算** | 均值 + 方差 → 中心化+标准化 | 均方根 → 仅标准化（无中心化） |
    | **可学习参数** | 2个：$\gamma$（缩放）、$\beta$（平移） | 1个：$\gamma$（缩放） |
    | **计算复杂度** | 稍高（多了均值计算步骤） | 更低（少一步均值计算，速度更快） |
    | **参数冗余度** | 略高（$\beta$可能带来冗余） | 更简洁（无$\beta$，减少参数数量） |
    | **训练稳定性** | 依赖均值中心化，适合小模型/小批次 | 去掉中心化，减少噪声干扰，适合大模型（T5是千亿级参数） |

    #### 核心原因
    1.  **适配大规模预训练**
        T5是超大参数量模型，训练时批次大、计算量高。RMSNorm少了“计算均值”的步骤，**计算速度更快、显存占用更低**，能提升大规模训练的效率。
    2.  **减少参数冗余，提升泛化性**
        LayerNorm的平移参数 $\beta$ 在很多任务中作用有限，反而会增加模型的参数总量。RMSNorm去掉 $\beta$ 后，参数更精简，不容易过拟合，能更好地适应不同的下游任务和数据集。
    3.  **避免均值带来的噪声**
        中心化（减均值）操作会让特征的“绝对位置”信息丢失一部分，而RMSNorm保留了特征的原始偏移，对于“文本到文本”的生成任务（T5的核心场景），能更好地保留语义的完整性。

    #### 类比
    - **LayerNorm**：像给一组数据“先归零，再缩放”，比如把成绩从 [50, 80, 90] 变成 [-1.2, 0.6, 1.2]，再调整到合适范围。
    - **RMSNorm**：像给一组数据“直接缩放”，比如把成绩从 [50, 80, 90] 变成 [0.5, 0.8, 0.9]，保留原始的相对差异，操作更简单。

#### 预训练任务
**[知识点]** 主要包括以下几个部分：
• 预训练任务: T5模型的预训练任务是 MLM，也称为BERT-style目标。具体来说，就是在输入文本中随机遮蔽15%的token，然后让模型预测这些被遮蔽的token。这个过程不需要标签，可以在大量未标注的文本上进行。
• 输入格式: 预训练时，T5将输入文本转换为"文本到文本"的格式。对于一个给定的文本序列，随机选择一些token进行遮蔽，并用特殊的占位符(token)替换。然后将被遮蔽的token序列作为模型的输出目标。
• 预训练数据集: T5 使用了自己创建的大规模数据集"Colossal Clean Crawled Corpus"(C4)，该数据集从Common Crawl中提取了大量干净的英语文本。C4数据集经过了一定的清洗，去除了无意义的文本、重复文本等。
**• 多任务预训练: T5 还尝试了将多个任务混合在一起进行预训练，而不仅仅是单独的MLM任务。这有助于模型学习更通用的语言表示。**
• 预训练到微调的转换: 预训练完成后，T5模型会在下游任务上进行微调。微调时，模型在任务特定的数据集上进行训练，并根据任务调整解码策略。

#### 大一统思想
**[知识点]** T5模型的一个核心理念是“大一统思想”，即所有的 NLP 任务都可以统一为文本到文本的任务

**[知识点]** 对于不同的NLP任务，每次输入前都会加上一个任务描述前缀，明确指定当前任务的类型。
- [QA] Q: 是预训练阶段还是微调阶段添加的？
> A: 在微调阶段添加，预训练阶段不添加任务描述前缀

# Task 8
## Decoder-Only PLM
**[知识点]** Decoder-Only 就是目前大火的 LLM 的基础架构，目前所有的 LLM 基本都是 Decoder-Only 模型（RWKV、Mamba 等非 Transformer 架构除外）。而引发 LLM 热潮的 ChatGPT，正是 Decoder-Only 系列的代表模型 GPT 系列模型的大成之作。而目前作为开源 LLM 基本架构的 LLaMA 模型，也正是在 GPT 的模型架构基础上优化发展而来。

### GPT
GPT，即 Generative Pre-Training Language Model，是由 OpenAI 团队于 2018年发布的预训练语言模型。

#### 模型架构——Decoder Only
![GPT](./pics/GPT.png) #TODO

BERT 选择了可训练的全连接层作为位置编码
- [QA] Q: 展开讲讲。
> A: BERT 是Encoder-only 模型，专注于文本理解任务（比如情感分析、NER），处理的句子长度相对固定（预训练时一般限定最大长度 512），所以可训练位置编码的 “短板”（不能处理超长句）完全不影响，反而有 2 个明显优势：
    - 更贴合文本理解：没有固定公式的约束，模型能根据具体的语言数据学出最适合的位置表达 —— 比如在中文里，“第 2 位” 的位置向量，模型会训到更贴合中文的语序规律，比正余弦的 “通用公式” 更适配；
    - 实现更简单：不用写正余弦的计算逻辑，只需要随机初始化位置向量，加一个全连接层，就能和模型其他部分一起训练，代码层面更简洁，训练时也能和词向量、注意力层的参数协同优化，整体效果更好。

第一代 GPT 模型和原始 Transformer 模型类似，选择了 12层解码器层，但是在解码器层的内部，相较于 Transformer 原始 Decoder 层的双注意力层设计，GPT 的 Decoder 层反而更像 Encoder 层一点。由于不再有 Encoder 的编码输入，Decoder 层仅保留了一个带掩码的注意力层，并且将 LayerNorm 层从 Transformer 的注意力层之后提到了注意力层之前。hidden_states 输入 Decoder 层之后，会先进行 LayerNorm，再进行掩码注意力计算，然后经过残差连接和再一次 LayerNorm 进入到 MLP 中并得到最后输出。
- [QA] Q: 列表对比BERT和GPT，架构上分别都有哪些区别？然后这样设计的原因？
> A: 
1) BERT vs GPT-1 核心架构差异（精准对应层/组件设计）

| 对比维度                | BERT（Encoder-only）| GPT-1（Decoder-only，基于Transformer Decoder改造） |
|-------------------------|-------------------------------------------------|----------------------------------------------------|
| **核心架构**            | 纯Transformer Encoder堆叠（Base版12层）| 改造后的Transformer Decoder堆叠（12层，无Encoder）|
| **注意力层设计**        | 单一层**双向自注意力**（无掩码），能看到当前token前后所有token | 单一层**掩码自注意力（因果掩码）**，仅保留Transformer Decoder的第一个注意力层，移除交叉注意力层；仅能看到当前token及之前的token，看不到后续token |
| **LayerNorm位置**       | 「Pre-Norm」结构（注意力/MLP前先做LayerNorm）| 同Pre-Norm结构（GPT-1率先固化，比原始Transformer更优）|
| **残差连接配合方式**    | 注意力/MLP层后直接残差相加，再衔接下一层Norm    | 与BERT一致（Norm→计算→残差），无差异                |
| **位置编码方式**        | 可训练的位置向量（全连接层融合，无固定公式）| 正余弦位置编码（延续Transformer，适配生成的变长需求） |
| **输入处理逻辑**        | 支持双句子拼接输入（加`[CLS]`/`[SEP]`标记），适配配对任务 | 单句子串行输入，无特殊配对标记，按生成顺序逐个喂入token |
| **模型输出形式**        | `[CLS]`token的全局语义向量（适配分类/匹配等理解任务） | 每个token的下一个token概率分布（适配自回归生成任务） |


也就是对一个输入的 hidden_states，会通过三个参数矩阵来生成 query、key 和 value，而不再是像 Transformer 中的 Decoder 那样由 Encoder 输出作为 key 和 value。
- [QA] Q: 展开讲讲。
> A: GPT 会将前文的 hidden_states（维度：[seq_len, d_model]），分别通过 3 个独立的可训练参数矩阵（Wq、Wk、Wv），线性变换生成 3 个维度相同的向量：Query、Key、Value,全部来自模型自身生成的前文 hidden_states，没有任何外部信息输入；
整个过程是 **“自回归”的**：每生成一个词，就把它加入到 hidden_states 中，作为下一次生成的 “前文”，循环往复，直到生成完整文本；
没有 Encoder，因此无需关注 “外部原文”，只需要关注 “自身前文”，完美适配纯文本生成任务。

另外⼀个结构上的区别在于，GPT 的 MLP 层没有选择线性矩阵来进⾏特征提取，⽽是选择了两个⼀维卷积核来提
取，不过，从效果上说这两者是没有太⼤区别的。
- [QA] Q: 展开讲讲。
> A: 无论是线性矩阵（全连接层）还是两个一维卷积核（1D Conv），本质都是对hidden_states做「线性变换 + 特征提取」，只是 “提取信息的方式” 像 “整本书按页整理” 和 “按章节批量整理” 的区别，最终都能拿到需要的核心信息，因此效果差异极小；GPT 选择一维卷积，更多是从计算效率和实现便捷性出发，而非效果提升。

#### 预训练任务——CLM

最传统也最直接的预训练任务——因果语言模型，Casual Language Model，下简称 CLM。

N-gram 语言模型是基于前 N 个 token 来预测下一个 token，CLM 则是基于一个自然语言序列的前面所有 token 来预测下一个 token，

#### GPT 系列模型的发展

OpenAI 一直坚信 Decoder-Only 的模型结构和“体量即正义”的优化思路。
从 GPT-1 到 GPT-3 的模型结构、预训练语料大小的变化：

模型 | Decoder Layer | Hidden_size | 注意力头数 | 注意力维度 | 总参数量 | 预训练语料 
---- | --------------|------------|------------|----------|----------|----------
GPT-1|12|3072|12|768|0.12B|5GB
GPT-2|48|6400|25|1600|1.5B|40GB
GPT-3|96|49152|96|12288|175B|570GB

GPT-2 的模型结构将 Post-Norm 改为了 Pre-Norm（也就是先进行 LayerNorm 计算，再进入注意力层计算）。这些改动的核心原因在于，由于模型层数增加、体量增大，梯度消失和爆炸的风险也不断增加，为了使模型梯度更稳定对上述结构进行了优化。

GPT-2 的另一个重大突破是以 zero-shot（零样本学习）为主要目标，也就是不对模型进行微调，直接要求模型解决任务。例如，在传统的预训练-微调范式中，我们要解决一个问题，一般需要收集几百上千的训练样本，在这些训练样本上微调预训练语言模型来实现该问题的解决。而 zero-shot 则强调不使用任何训练样本，直接通过向预训练语言模型描述问题来去解决该问题。zero-shot 的思路自然是比预训练-微调范式更进一步、更高效的自然语言范式，但是在 GPT-2 的时代，模型能力还不足够支撑较好的 zero-shot 效果，在大模型时代，zero-shot 及其延伸出的 few-shot（少样本学习）才开始逐渐成为主流。

few-shot 也被称为上下文学习（In-context Learning），即让模型从提供的上下文中的示例里学习问题的解决方法。
- [QA] Q: 为什么会有用？
> A: Few-shot（上下文学习）之所以有用，本质是 GPT3 这类大参数量 Decoder-only 模型，在海量语料的预训练中，已经学会了「人类语言的所有底层规律」（语法、逻辑、推理、任务范式），Few-shot 给的少量示例，只是「唤醒 / 激活模型的这些已有能力，告诉模型「现在要做什么任务、按什么格式做」，而非教模型「重新学新知识」。GPT3 的 Few-shot 能生效，核心是它满足 **“大参数量 + 海量预训练 + Decoder-only 自回归架构”** 三个条件，这三个条件让模型具备了「存储通用规律 + 理解上下文范式 + 精准匹配任务」的能力。
Few-shot和“传统微调”的本质区别（更懂为什么Few-shot高效）
很多人会把Few-shot和传统微调混淆，其实二者完全不同，这也能进一步解释Few-shot的价值：

| 对比维度       | Few-shot（上下文学习）| 传统微调（Fine-tuning）|
|----------------|---------------------------------------------|---------------------------------------------|
| 核心逻辑       | 唤醒模型已有能力，定任务格式                | 教模型学习新任务，更新模型参数              |
| 是否修改参数   | 不修改（纯推理阶段，模型参数全程不变）| 必须修改（训练阶段更新预训练权重）|
| 数据需求       | 极少（1~5个示例）| 较多（至少几十上百个标注数据）|
| 耗时/成本      | 极低（直接推理，无需训练）| 较高（需要训练、调参、验证）|
| 适用场景       | 快速适配各类小任务、小众任务                | 适配核心业务任务，追求极致效果              |

简单说：**传统微调是“给模型补课，重新学知识点”，Few-shot是“给模型指方向，让它用已会的知识点做题”**。


在 GPT 系列模型的基础上，通过引入预训练-指令微调-人类反馈强化学习的三阶段训练，OpenAI 发布了跨时代的 ChatGPT，

### LLaMA

LLaMA模型是由Meta（前Facebook）开发的一系列大型预训练语言模型。从LLaMA-1到LLaMA-3，LLaMA系列模型

#### 模型架构——Decoder Only

LLaMA模型的整体结构与GPT系列模型类似，只是在模型规模和预训练数据集上有所不同。
![LLaMA](./pics/LLaMA.png)
在完成masked self-attention层之后，hidden_states会进入MLP层。在这个多层感知机层中，模型通过两个全连接层对hidden_states进行进一步的特征提取。第一个全连接层将hidden_states映射到一个中间维度，然后通过激活函数进行非线性变换，增加模型的非线性能力。第二个全连接层则将特征再次映射回原始的hidden_states维度。

最后，经过多个decoder block的处理，hidden_states会通过一个线性层进行最终的映射，这个线性层的输出维度与词表维度相同。这样，模型就可以根据hidden_states生成目标序列的概率分布，进而通过采样或贪婪解码等方法，生成最终的输出序列。这一过程体现了LLaMA模型强大的序列生成能力。
- [QA] Q: 需要解释下。
> A: 经过多个Decoder Block处理后的`hidden_states`，包含了模型对“前文该生成什么后续内容”的完整语义信息，但它还是「模型内部能理解的高维向量」（无法直接对应人类语言）。而最后的线性层，本质是 **“翻译官”**，把高维语义向量「映射翻译」成词表中每个token的概率值，再通过解码策略（贪婪解码/采样），从概率值中选出合适的token，逐词拼接成最终的人类可理解文本，这就是LLaMA实现序列生成的最后一步，也是生成能力的最终落地。
经过层层Decoder Block（掩码自注意力+MLP）的处理，`hidden_states`的形状通常是`[seq_len, d_model]`（`seq_len`是前文序列长度，`d_model`是模型隐藏层维度，比如LLaMA-7B的`d_model=4096`）。
   - 它不是“单个向量”，而是“每个前文token对应的高维语义向量集合”；
   - 其中，**最后一个token对应的`hidden_state`（即`hidden_states[-1]`）是核心**——它融合了所有前文的语义信息，专门用于预测“下一个该生成的token”（LLaMA是自回归生成，逐词推进，每次只预测下一个token）。
**两个核心组件的作用**
   -  线性层（最终映射层）：输入是`d_model`维的隐藏向量，输出是「词表维度」（比如LLaMA的词表大小约32k），作用是“高维向量→token概率”的映射；
   -  解码策略（贪婪解码/采样）：输入是token概率分布，作用是“概率值→具体token”的选择，决定生成文本的流畅度和多样性。

    常见的有2种核心策略：

    贪婪解码（Greedy Decoding）——“选最保险、概率最高的”
    逻辑：直接选择概率分布中概率值最大的那个token，作为下一个生成的token。

    对应例子：直接选概率0.8的“公园”，续写句子为「“今天天气很好，我打算去公园”」；
    优点：生成速度快，结果稳定，不容易出现语法错误（“最保险”）；
    缺点：多样性差，容易生成“千篇一律”的文本，比如每次续写都是“公园”，缺乏新意，也可能陷入循环（比如“公园公园公园”）。
    采样（Sampling）——“随机选，但偏向概率高的”
    逻辑：不强制选概率最高的，而是以“概率值为权重”，进行随机抽样（概率越高的token，被抽中的概率越大），常见的有“温度采样”“Top-K采样”“Top-P采样”。

    对应例子：可能抽中0.8的“公园”，也可能抽中0.15的“爬山”，甚至偶尔抽中0.04的“超市”，续写句子可能是「“今天天气很好，我打算去爬山”」或「“今天天气很好，我打算去超市”」；
    优点：多样性强，生成的文本更有新意，更接近人类的自然表达；
    缺点：有一定概率抽中低概率的“奇怪token”（比如“电脑”），导致语法错误或逻辑不通（比如「“今天天气很好，我打算去电脑”」）。

#### LLaMA模型的发展历程

**LLaMA-1 系列**：开源性和优异性能
**LLaMA-2 系列**：分组查询注意力机制（Grouped-Query Attention, GQA）
**LLaMA-3 系列**：支持8K长文本，并采用了编码效率更高的tokenizer

### GLM

GLM 系列模型是由智谱开发的主流中文 LLM 之一，包括 ChatGLM1、2、3及 GLM-4 系列模型。

#### 模型架构-相对于 GPT 的略微修正

核心思路是在传统 CLM 预训练任务基础上，加入 MLM 思想，从而构建一个在 NLG 和 NLU 任务上都具有良好表现的统一模型。

在整体模型结构上，GLM 和 GPT 大致类似，均是 Decoder-Only 的结构，仅有三点细微差异：

1. 使用 Post Norm 而非 Pre Norm。Post Norm 是指在进行残差连接计算时，先完成残差计算，再进行 LayerNorm 计算；而类似于 GPT、LLaMA 等模型都使用了 Pre Norm，也就是先进行 LayerNorm 计算，再进行残差的计算。相对而言，Post Norm 由于在残差之后做归一化，对参数正则化的效果更强，进而模型的鲁棒性也会更好；Pre Norm相对于因为有一部分参数直接加在了后面，不需要对这部分参数进行正则化，正好可以防止模型的梯度爆炸或者梯度消失。因此，对于更大体量的模型来说，一般认为 Pre Norm 效果会更好。但 GLM 论文提出，使用 Post Norm 可以避免 LLM 的数值错误（虽然主流 LLM 仍然使用了 Pre Norm）；

2. 使用单个线性层实现最终 token 的预测，而不是使用 MLP；这样的结构更加简单也更加鲁棒，即减少了最终输出的参数量，将更大的参数量放在了模型本身；

3. 激活函数从 ReLU 换成了 GeLUs。ReLU 是传统的激活函数，其核心计算逻辑为去除小于 0的传播，保留大于 0的传播；GeLUs 核心是对接近于 0的正向传播，做了一个非线性映射，保证了激活函数后的非线性输出，具有一定的连续性。
- [QA] Q: 展开解释。
> A: 
1. Pre Norm 是 **“防患于未然”，牺牲一点正则化效果，换大模型的训练稳定性；
Post Norm 是“事后严格把关”**，牺牲一点大模型训练友好度，换更强的鲁棒性和更少的数值错误。
2. 直接用一个线性层做映射 —— 输入是d_model维的hidden_states，输出直接是词表维度的logits（原始得分），无任何额外层；用最小的参数量代价完成 token 预测，把更多的模型容量（参数量）分配到对生成能力更关键的Decoder Block 内部（注意力 + MLP）。
3. GELU几乎是ReLU的**“全面升级版”**——除了计算速度略慢于ReLU（可忽略），在**特征保留、梯度传播、非线性表达**上都更优，能显著提升模型的生成效果和训练稳定性，因此目前所有主流LLM（GPT3/4、LLaMA、GLM、文心一言等）都已采用GELU作为激活函数。

#### 预训练任务-GLM
 GLM（General Language Model，通用语言模型）任务结合了自编码思想和自回归思想的预训练方法。自编码=MLM（学习被删除的token），自回归=CLM（按顺序重建连续token）

 #### GLM 家族的发展
 智谱于 23年 3月发布了第一个中文开源 LLM ChatGLM-6B
 在 23年 6月，智谱就开源了 ChatGLM2-6B：将上下文长度扩展到了 32K；模型架构就基本回归了 LLaMA 架构，引入 MQA 的注意力机制，
 ChatGLM3-6B 发布于 23年 10月：开始支持函数调用与代码解释器。
 2024年 1月，智谱发布了支持 128K 上下文，包括多种类型的 GLM-4 系列模型。


