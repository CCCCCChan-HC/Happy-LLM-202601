# Task6
### 心得&笔记&疑问&解答：
## Encoder-Only PLM
**[知识点]** 针对 Encoder、Decoder 的特点，**引入 ELMo 的预训练思路**，开始出现不同的、对 Transformer 进行优化的思路。
- [QA] Q：引入ELMo预训练思路 此处是什么意思？
    > A: ELMo 的诞生标志着预训练+微调范式的诞生。ELMo 模型基于双向 LSTM 架构，在训练数据上基于语言模型进行预训练，再针对下游任务进行微调，表现出了更加优越的性能，将 NLP 领域导向预训练+微调的研究思路。
### BERT
**[知识点]** BERT，全名为 Bidirectional Encoder Representations from Transformers，发布于论文《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》
**[知识点]** 出现了如 MacBERT、BART 等基于 BERT 进行优化提升的模型。


#### 沿承的核心思想包括：
- Transformer 架构。通过将 Encoder 结构进行堆叠，扩大模型参数
- 预训练+微调范式。掩码语言模型（Masked Language Model，MLM），能捕捉深层双向语义关系
#### 模型架构—Encoder Only
![BERT模型结构](./pics/cap02.png)
**[知识点]** 没有加入对特定任务的 Decoder。为适配各种 NLU 任务，在模型的最顶层加入了一个分类头 prediction_heads，用于将多维度的隐藏状态通过线性层转换到分类维度（例如，如果一共有两个类别，prediction_heads 输出的就是两维向量）

**[知识点]** BERT 有两种规模的模型，分别是 base 版本（12层 Encoder Layer，768 的隐藏层维度，总参数量 110M），large 版本（24层 Encoder Layer，1024 的隐藏层维度，总参数量 340M）。通过Encoder 编码之后的最顶层 hidden_states 最后经过 prediction_heads 就得到了最后的类别概率，经过 Softmax 计算就可以计算出模型预测的类别。

> BERT 采用 WordPiece 作为分词方法。WordPiece 是一种基于统计的子词切分算法，其核心在于将单词拆解为子词（例如，"playing" -> ["play", "##ing"]）。其合并操作的依据是最大化语言模型的似然度。对于中文等非空格分隔的语言，通常将单个汉字作为原子分词单位（token）处理。

**[知识点]** BERT 所使用的激活函数是 GELU 函数，全名为高斯误差线性单元激活函数.GELU 的计算方式为：

$$GELU(x) = 0.5x(1 + tanh(\sqrt{\frac{2}{\pi}})(x + 0.044715x^3))$$

**[知识点]** GELU 的核心思路为将随机正则的思想引入激活函数，通过输入自身的概率分布，来决定抛弃还是保留自身的神经元。
- [QA] Q：怎么理解GELU的核心思想？
    > A: 
    1. 传统激活函数（比如 ReLU）：是 “固定开关”—— 输入大于 0，员工就干活；输入小于 0，员工就摸鱼（不管其他情况）；
    2. GELU 激活函数：是 “随机开关”—— 员工是否干活，由 “输入的概率” 决定：
    比如输入是 “写报告”，GELU 会先算 “这个输入需要员工干活的概率”（比如概率 = 0.8）；
    然后扔一个 “概率骰子”：如果骰子结果≤0.8，员工就干活；如果 > 0.8，员工就摸鱼；
    而且这个 “概率” 是根据输入自身的大小动态变化的—— 输入越重要（数值越大），干活的概率越高；输入越不重要（数值越小），干活的概率越低。
    3. 核心思想：
    GELU 把 “随机正则（比如 Dropout）” 和 “激活函数” 合二为一 —— 既让模型 “选择性激活神经元”（像 ReLU 一样做特征筛选），又让模型 “随机抛弃部分神经元”（像 Dropout 一样防止过拟合），相当于 “让员工既按能力干活，又随机轮休”，既提高效率又避免疲劳（过拟合）。

**[知识点]** BERT 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数
- [QA] Q：为什么这么做？
    > A: 通过可训练的参数来拟合相对位置，相对而言比 Transformer 使用的绝对位置编码 Sinusoidal 能够拟合更丰富的相对位置信息，但是，这样也增加了不少模型参数，同时完全无法处理超过模型训练长度的输入（例如，对 BERT 而言能处理的最大上下文长度是 512 个 token）。

#### 预训练任务-MLM+NSP
**[知识点]** NSP（Next Sentence Prediction，下一句预测）
**[知识点]** 通过将预训练和微调分离，完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上。
-> 因此，可以进一步扩大模型参数和预训练数据量。
-> 预训练数据的核心要求即是需要极大的数据规模（数亿 token）
-> 预训练数据一定是从无监督的语料中获取
-> 统的预训练任务都是 LM 的原因(LM 使用上文预测下文的方式可以直接应用到任何文本中)
-> LM 预训练任务的一大缺陷在于，其直接拟合从左到右的语义关系，但忽略了双向的语义关系
-> MLM诞生

**[知识点]** MLM 模拟的是“完形填空”
- 在一个文本序列中随机遮蔽部分 token，然后将所有未被遮蔽的 token 输入模型，要求模型根据输入预测被遮蔽的 token
- MLM存在其固有缺陷。在下游任务微调和推理时，其实是不存在我们人工加入的\<MASK\>的。

**[知识点]** 对 MLM 的策略进行了改进：
- 随机选择训练语料中 15% 的 token 用于遮蔽
- 80% 的概率被遮蔽
- 10% 的概率被替换为任意一个 token（核心意义在于迫使模型保持对上下文信息的学习。因为如果全部遮蔽的话，模型仅需要处理被遮蔽的位置，从而仅学习要预测的 token 而丢失了对上下文的学习。通过引入部分随机 token，模型无法确定需要预测的 token，从而被迫保持每一个 token 的上下文表征分布，从而具备了对句子的特征表示能力。且由于随机 token 的概率很低，其并不会影响模型实质的语言理解能力。）
- 10% 的概率保持不变（为了消除预训练和微调的不一致）

**[知识点]** NSP 任务的核心思路是要求模型判断一个句对的两个句子是否是连续的上下文
**[知识点]** NSP 的正样本可以从无监督语料中随机抽取任意连续的句子，而负样本可以对句子打乱后随机抽取（只需要保证不要抽取到原本就连续的句子就行）

**[知识点]** 训练的超参数也是值得关注的，BERT 的训练语料共有 13GB 大小，其在 256 的 batch size 上训练了 1M 步（40 个 Epoch）。而相较而言，LLM 一般都只会训练一个 Epoch，且使用远大于 256 的 batch size。
- [QA] Q: 这句话怎么理解？
> A: 类比就是，BERT 预训练时，每次做 256 道题，改完错再写下一批，一共做40批；而LLM一般只做一批，但是题目量远大于BERT

#### 下游任务微调
**[知识点]** BERT 设计了更通用的输入和输出层来适配多任务下的迁移学习。对每一个输入的文本序列，BERT 会在其首部加入一个特殊 token \<CLS\>。在后续编码中，该 token 代表的即是整句的状态

**[知识点]** 所谓微调，其实和训练时更新模型参数的策略一致，只不过在特定的任务、更少的训练数据、更小的 batch_size 上进行训练，更新参数的幅度更小。

### RoBERTa 
由 Facebook 发布
#### 优化一：去掉 NSP 预训练任务
**[知识点]** 设置了四个实验组：

    1. 段落构建的 MLM + NSP：BERT 原始预训练任务，输入是一对片段，每个片段包括多个句子，来构造 NSP 任务；
    2. 文档对构建的 MLM + NSP：一个输入构建一对句子，通过增大 batch 来和原始输入达到 token 等同；
    3. 跨越文档的 MLM：去掉 NSP 任务，一个输入为从一个或多个文档中连续采样的完整句子，为使输入达到最大长度（512），可能一个输入会包括多个文档；
    4. 单文档的 MLM：去掉 NSP 任务，且限制一个输入只能从一个文档中采样，同样通过增大 batch 来和原始输入达到 token 等同

后两组显著优于前两组，RoBERTa 在预训练中去掉了 NSP

**[知识点]** 动态遮蔽策略，让每一个 Epoch 的训练数据 Mask 的位置都不一致
#### 优化二：更大规模的预训练数据和预训练步长
**[知识点]** RoBERTa 认为更大的 batch size 既可以提高优化速度，也可以提高任务结束性能。
- [QA] Q: 为什么这么做？更大的预训练数据、更大的预训练步长的重要意义？
> A: 
1. 更大 batch size 的好处（对应 “每次多做几道题”）：
提高优化速度：
普通训练（batch size=256）= 每次做 256 道题，改完错再写下一批；
大 batch（比如 batch size=8192）= 每次做 8192 道题，改一次错就能覆盖更多题型，模型 “学知识的效率变高”，不用重复做同类题。
提高最终性能：
大 batch 能一次性看到更多 “不同类型的题”（比如同时练语文、数学、英语），模型学到的 “知识更全面”，不会偏科 —— 相当于你刷题时一次做语数外各 10 道，比每次只做 10 道数学，知识覆盖更广。
2. 更大预训练数据的意义（对应 “用更多题本刷题”）：
普通 BERT 用 13GB 语料 = 只刷一本《高考必刷题》；
大模型用 TB 级语料 = 刷《高考必刷题 + 历年真题 + 模拟卷 + 课外拓展题》—— 题越多，模型见的 “语言现象”（比如不同句式、生僻词、逻辑关系）越全，学到的 “语言能力” 越强，做新题（下游任务）时更会举一反三。
3. 更长预训练步长的意义（对应 “刷更多轮题”）：
步长 =“刷一轮题” 的次数：BERT 刷 1M 步 = 把题本做了 40 遍；
更长步长 = 把题本刷更多遍 —— 模型能把 “知识从记不住→记住→熟练→融会贯通”，比如第一次刷题只懂 “主谓宾”，刷 10 遍后能懂 “倒装句、省略句”，刷 100 遍后能熟练用复杂句式。
#### 优化三：更大的 bpe 词表
**[知识点]** BPE，即 Byte Pair Encoding，字节对编码，是指以子词对作为分词的单位。以字为基本单位的中文，一般会按照字节编码进行切分。
**[知识点]** 一般来说，BPE 编码的词典越大，编码效果越好。
### ALBERT
### 优化一：将 Embedding 参数进行分解
**[知识点]** Embedding 层的参数矩阵维度为 $V*H$，此处的 V 为词表大小 30K，H 即为隐藏层大小 1024，也就是 Embedding 层参数达到了 30M。
**[知识点]** Embedding 层输出的向量是我们对文本 token 的稠密向量表示，从 Word2Vec来看，这种词向量并不需要很大的维度。
**[知识点]** 在 Embedding 层的后面加入一个线性矩阵进行维度变换。ALBERT 设置了 Embedding 层的输出为 128，因此在 Embedding 层后面加入了一个 $128 * 1024$ 的线性矩阵来将 Embedding 层的输出再升维到隐藏层大小。

### 优化二：跨层进行参数共享
**[知识点]** 让各个 Encoder 层共享模型参数，来减少模型的参数量。
**[知识点]** 仅初始化了一个Encoder层。在计算过程中，仍然会进行24次计算，但是每一次计算都是经过这一个Encoder层。因此，虽然是24个Encoder 计算的模型，但只有一层 Encoder参数，从而大大降低了模型参数量。在这样的情况下，就可以极大程度地扩大隐藏层维度，实现一个更宽但参数量更小的模型。

**[知识点]** 不足：训练和推理时的速度相较 BERT 还会更慢

#### （3）优化三：提出 SOP 预训练任务

**[知识点]** 改进 NSP，增加其难度，优化模型的预训练

**[知识点]** 改进是，正例同样由两个连续句子组成，但负例是将这两个的顺序反过来。模型不仅要拟合两个句子之间的关系，更要学习其顺序关系

**[知识点]** 还有许多从其他更高角度对 BERT 进行优化的后起之秀，包括进一步改进了预训练任务的 ERNIE、对 BERT 进行蒸馏的小模型 DistilBERT、主打多语言任务的 XLM 等，本文就不再一一赘述。

# Task 7
## Encoder-Decoder PLM
**[知识点]** BERT 也存在一些问题，例如 MLM 任务和下游任务微调的不一致性，以及无法处理超过模型训练长度的输入等问题。
- [QA] Q: 展开讲讲问题和原因
> A: 
1. 预训练和微调的 “输入规则” 不一样
2. MLM 任务只要求模型根据上下文预测单个token，关注的是局部上下文的语义关联；而下游任务（如文本摘要、问答）往往需要全局语义理解、长距离依赖建模，两者的学习目标存在偏差。
3. 静态掩码
4. 输入长度问题见task 6.

### T5
**[知识点]** T5（Text-To-Text Transfer Transformer）是由 Google 提出,将所有 NLP 任务统一表示为文本到文本的转换问题
#### 模型结构：Encoder-Decoder
**[知识点]** 编码器用于处理输入文本，解码器用于生成输出文本。编码器和解码器之间通过注意力机制进行信息交互.
![T5 模型详细结构](./pics/t5.png)
**[知识点]** Transformer 部分又分为 EncoderLayers 和 DecoderLayers 两部分，由小的Block组成，每个 Block 包含了多头注意力机制、前馈神经网络和 Norm 层。Block 的设计可以使模型更加灵活，根据任务的复杂程度和数据集的大小来调整 Block 的数量和层数。

**[知识点]** 前馈神经网络用于处理特征的非线性变换
- [QA] Q: 这句话怎么具体理解
> A: 在自注意力机制提取的 “注意力特征” 基础上，通过线性变换+非线性激活的组合，让模型突破线性表达的局限，学习到文本中更复杂的语义、语法和逻辑关系。

**[知识点]** 在 Decoder 中还包含了 Encoder-Decoder Attention 结构，在位置编码和 Mask 机制上有所不同。
![Encoder和Decoder](./pics/encoder-decoder.png)

**[知识点]** 与原始 Transformer 模型不同，T5 模型的LayerNorm 采用了 RMSNorm，通过计算每个神经元的均方根（Root Mean Square）来归一化每个隐藏层的激活值。RMSNorm 的参数设置与Layer Normalization 相比更简单，只有一个可学参数，可以更好地适应不同的任务和数据集。RMSNorm函数可以用以下数学公式表示：

$$
\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}x_i^2 + \epsilon}} \cdot \gamma
$$

其中：
- $x_i$ 是输入向量的第 $i$ 个元素
- $\gamma$ 是可学习的缩放参数
- $n$ 是输入向量的维度数量
- $\epsilon$ 是一个小常数，用于数值稳定性（以避免除以零的情况）

这种归一化有助于通过确保权重的规模不会变得过大或过小来稳定学习过程，这在具有许多层的深度学习模型中特别有用。

- [QA] Q: 对比一下LayerNorm
> A: **核心简化点**：
- 去掉了**减去均值**的步骤，只做“标准化”，不做“中心化”
- 只有 **1个可学习参数**：$\gamma$（缩放因子），没有平移参数 $\beta$
    #### LayerNorm 与 RMSNorm 的核心区别对比
    | 对比维度 | 原始Transformer的LayerNorm | T5的RMSNorm |
    | :--- | :--- | :--- |
    | **核心计算** | 均值 + 方差 → 中心化+标准化 | 均方根 → 仅标准化（无中心化） |
    | **可学习参数** | 2个：$\gamma$（缩放）、$\beta$（平移） | 1个：$\gamma$（缩放） |
    | **计算复杂度** | 稍高（多了均值计算步骤） | 更低（少一步均值计算，速度更快） |
    | **参数冗余度** | 略高（$\beta$可能带来冗余） | 更简洁（无$\beta$，减少参数数量） |
    | **训练稳定性** | 依赖均值中心化，适合小模型/小批次 | 去掉中心化，减少噪声干扰，适合大模型（T5是千亿级参数） |

    #### 核心原因
    1.  **适配大规模预训练**
        T5是超大参数量模型，训练时批次大、计算量高。RMSNorm少了“计算均值”的步骤，**计算速度更快、显存占用更低**，能提升大规模训练的效率。
    2.  **减少参数冗余，提升泛化性**
        LayerNorm的平移参数 $\beta$ 在很多任务中作用有限，反而会增加模型的参数总量。RMSNorm去掉 $\beta$ 后，参数更精简，不容易过拟合，能更好地适应不同的下游任务和数据集。
    3.  **避免均值带来的噪声**
        中心化（减均值）操作会让特征的“绝对位置”信息丢失一部分，而RMSNorm保留了特征的原始偏移，对于“文本到文本”的生成任务（T5的核心场景），能更好地保留语义的完整性。

    #### 类比
    - **LayerNorm**：像给一组数据“先归零，再缩放”，比如把成绩从 [50, 80, 90] 变成 [-1.2, 0.6, 1.2]，再调整到合适范围。
    - **RMSNorm**：像给一组数据“直接缩放”，比如把成绩从 [50, 80, 90] 变成 [0.5, 0.8, 0.9]，保留原始的相对差异，操作更简单。

#### 预训练任务
**[知识点]** 主要包括以下几个部分：
• 预训练任务: T5模型的预训练任务是 MLM，也称为BERT-style目标。具体来说，就是在输入文本中随机遮蔽15%的token，然后让模型预测这些被遮蔽的token。这个过程不需要标签，可以在大量未标注的文本上进行。
• 输入格式: 预训练时，T5将输入文本转换为"文本到文本"的格式。对于一个给定的文本序列，随机选择一些token进行遮蔽，并用特殊的占位符(token)替换。然后将被遮蔽的token序列作为模型的输出目标。
• 预训练数据集: T5 使用了自己创建的大规模数据集"Colossal Clean Crawled Corpus"(C4)，该数据集从Common Crawl中提取了大量干净的英语文本。C4数据集经过了一定的清洗，去除了无意义的文本、重复文本等。
**• 多任务预训练: T5 还尝试了将多个任务混合在一起进行预训练，而不仅仅是单独的MLM任务。这有助于模型学习更通用的语言表示。**
• 预训练到微调的转换: 预训练完成后，T5模型会在下游任务上进行微调。微调时，模型在任务特定的数据集上进行训练，并根据任务调整解码策略。

#### 大一统思想
**[知识点]** T5模型的一个核心理念是“大一统思想”，即所有的 NLP 任务都可以统一为文本到文本的任务

**[知识点]** 对于不同的NLP任务，每次输入前都会加上一个任务描述前缀，明确指定当前任务的类型。
- [QA] Q: 是预训练阶段还是微调阶段添加的？
> A: 在微调阶段添加，预训练阶段不添加任务描述前缀


